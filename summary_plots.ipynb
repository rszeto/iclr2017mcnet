{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import imageio\n",
    "import glob\n",
    "import argparse\n",
    "from pprint import pprint\n",
    "from warnings import warn\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "ARCHS = ['mcnet', 'prednet']\n",
    "\n",
    "RESULTS_ROOTS = {\n",
    "    'mcnet': os.path.join('results', 'quantitative', 'MNIST'),\n",
    "    'prednet': os.path.join('prednet', 'mnist_results', 'quantitative', 'MNIST')\n",
    "}\n",
    "\n",
    "IMAGE_ROOTS = {\n",
    "    'mcnet': os.path.join('results', 'images', 'MNIST'),\n",
    "    'prednet': os.path.join('prednet', 'mnist_results', 'images', 'MNIST')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Display utils\n",
    "'''\n",
    "\n",
    "def get_gif_path(arch, dataset_label, model_label, video_idx):\n",
    "    return os.path.join(IMAGE_ROOTS[arch], 'data=%s' % dataset_label,\n",
    "                        'model=%s' % model_label, 'both_%04d.gif' % video_idx)\n",
    "\n",
    "def show_image_row(image_paths, image_titles):\n",
    "    assert(len(image_paths) == len(image_titles))\n",
    "    html = '<table><tr>'\n",
    "    for i in xrange(len(image_titles)):\n",
    "        html += '<td style=\"text-align:center\">%s</td>' % str(image_titles[i])\n",
    "    html += '</tr><tr>'\n",
    "    for i in xrange(len(image_paths)):\n",
    "        html += '<td><img src=\"%s\"></td>' \\\n",
    "            % (image_paths[i])\n",
    "    html += '</tr></table>'\n",
    "    \n",
    "    display(HTML(html))\n",
    "\n",
    "def visualize_videos(arch, dataset_label, model_label, sorted_idx):\n",
    "    print('Worst videos:')\n",
    "    image_paths = [get_gif_path(arch, dataset_label, model_label, i) for i in sorted_idx[:5]]\n",
    "    show_image_row(image_paths, sorted_idx[:5])\n",
    "    print('Middle videos:')\n",
    "    mid_lower = len(sorted_idx) / 2 - 2\n",
    "    mid_upper = len(sorted_idx) / 2 + 3\n",
    "    image_paths = [get_gif_path(arch, dataset_label, model_label, i) for i in sorted_idx[mid_lower:mid_upper]]\n",
    "    show_image_row(image_paths, sorted_idx[mid_lower:mid_upper])\n",
    "    print('Best videos:')\n",
    "    image_paths = [get_gif_path(arch, dataset_label, model_label, i) for i in sorted_idx[-5:]]\n",
    "    show_image_row(image_paths, sorted_idx[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_experiments():\n",
    "    '''\n",
    "    Prepare a dictionary listing all the combinations of architecture, training set, and test set found\n",
    "    in the result paths. Also prints warnings for inconsistencies like missing output videos or experi-\n",
    "    ments only found for one architecture.\n",
    "    '''\n",
    "    \n",
    "    ret = {arch: {} for arch in ARCHS}\n",
    "    for arch in ARCHS:\n",
    "        dataset_labels = [x.replace('data=', '') for x in os.listdir(RESULTS_ROOTS[arch])]\n",
    "        if len(dataset_labels) == 0:\n",
    "            warn('No datasets found for architecture %s, skipping' % arch)\n",
    "        else:\n",
    "            for dataset_label in dataset_labels:\n",
    "                model_labels = [x.replace('model=', '') \n",
    "                               for x in os.listdir(os.path.join(RESULTS_ROOTS[arch], 'data=%s' % dataset_label))]\n",
    "                if len(model_labels) == 0:\n",
    "                    warn('No models found for architecture %s, data %s. Skipping' % (arch, dataset_label))\n",
    "                else:\n",
    "                    ret[arch][dataset_label] = model_labels\n",
    "    \n",
    "    # Check that each architecture has the same datasets\n",
    "    print('Comparing dataset %s with dataset %s' % tuple(ARCHS))\n",
    "    compare_experiment_structure(ret[ARCHS[0]], ret[ARCHS[1]])\n",
    "    # Check that images exist\n",
    "    for arch, arch_dict in ret.iteritems():\n",
    "        print('Checking result files for architecture %s' % arch)\n",
    "        print('Checking images for architecture %s' % arch)\n",
    "        check_images(arch, arch_dict)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def compare_experiment_structure(a, b):\n",
    "    '''\n",
    "    Check that the experiments in dictionary a match with those in b. Report inconsistencies\n",
    "    '''\n",
    "    for dataset_label in a.iterkeys():\n",
    "        for model_label in a[dataset_label]:\n",
    "            if dataset_label not in b.keys() or model_label not in b[dataset_label]:\n",
    "                warn('Found experiment (data=%s, model=%s) in first dictionary but not second' \\\n",
    "                     % (dataset_label, model_label))\n",
    "    for dataset_label in b.iterkeys():\n",
    "        for model_label in b[dataset_label]:\n",
    "            if dataset_label not in a.keys() or model_label not in a[dataset_label]:\n",
    "                warn('Found experiment (data: %s, model: %s) in second dictionary but not first' \\\n",
    "                     % (dataset_label, model_label))\n",
    "\n",
    "def check_result_files(arch, a):\n",
    "    '''\n",
    "    Check that exactly one npz file exists in each experiment in dictionary a\n",
    "    '''\n",
    "    for dataset_label in a.iterkeys():\n",
    "        for model_label in a[dataset_label]:\n",
    "            result_root = os.path.join(RESULTS_ROOTS[arch], 'data=%s' % dataset_label, 'model=%s' % model_label)\n",
    "            if not os.path.isdir(result_root):\n",
    "                warn('Result directory does not exist for (arch: %s, data: %s, model: %s)' \\\n",
    "                     % (arch, dataset_label, model_label))\n",
    "            else:\n",
    "                npz_files = [x for x in os.path.listdir(result_root) if x.endswith('.npz')]\n",
    "                if len(npz_files) == 0:\n",
    "                    warn('No .npz files found for (arch: %s, data: %s, model: %s)' \\\n",
    "                     % (arch, dataset_label, model_label))\n",
    "                elif len(npz_files) > 1:\n",
    "                    warn('Too many .npz files found for (arch: %s, data: %s, model: %s)' \\\n",
    "                     % (arch, dataset_label, model_label))\n",
    "                \n",
    "def check_images(arch, a):\n",
    "    '''\n",
    "    Check that the images for experiments in dictionary a exist\n",
    "    '''\n",
    "    for dataset_label in a.iterkeys():\n",
    "        for model_label in a[dataset_label]:\n",
    "            image_root = os.path.join(IMAGE_ROOTS[arch], 'data=%s' % dataset_label, 'model=%s' % model_label)\n",
    "            if not os.path.isdir(image_root):\n",
    "                warn('Image directory does not exist for (arch: %s, data: %s, model: %s)' \\\n",
    "                     % (arch, dataset_label, model_label))\n",
    "            else:\n",
    "                image_names = [x for x in os.listdir(image_root) if x.endswith('.gif')]\n",
    "                if len(image_names) == 0:\n",
    "                    warn('No images found for (arch: %s, data: %s, model: %s)' \\\n",
    "                         % (arch, dataset_label, model_label))\n",
    "\n",
    "                    \n",
    "def get_results(arch, dataset_label, model_label):\n",
    "    '''\n",
    "    Retrieve results array\n",
    "    '''\n",
    "    results_dir = os.path.join(RESULTS_ROOTS[arch], 'data=%s' % dataset_label, 'model=%s' % model_label)\n",
    "    if not os.path.isdir(results_dir):\n",
    "        raise RuntimeError('Path does not exist: %s' % results_dir)\n",
    "    results_file_arr = filter(lambda x: x.endswith('.npz'), os.listdir(results_dir))\n",
    "    if len(results_file_arr) == 0:\n",
    "        raise RuntimeError('No results file found in path %s' % results_dir)\n",
    "    elif len(results_file_arr) > 1:\n",
    "        raise RuntimeError('Too many results files found in path %s' % results_dir)\n",
    "    results_file_path = os.path.join(results_dir, results_file_arr[0])\n",
    "    return np.load(results_file_path, mmap_mode='r')\n",
    "\n",
    "\n",
    "def get_sorted_videos_by_metric(arch, dataset_label, model_label, metric):\n",
    "    '''\n",
    "    Get the list of videos sorted by the sum of the given metric over all time steps\n",
    "    '''\n",
    "    results = get_results(arch, dataset_label, model_label)\n",
    "    metric_vals = results[metric]\n",
    "    metric_aucs = np.sum(metric_vals, axis=1)\n",
    "    sorted_idx = np.argsort(metric_aucs)\n",
    "    return sorted_idx, metric_aucs[sorted_idx]\n",
    "\n",
    "\n",
    "def plot_sorted_auc(ax, arch, dataset_label, model_label, metric, label, max_value=1.0):\n",
    "    '''\n",
    "    Plot ranked AUC plot. max_value specifies the maximum area under the plot if plotting normalized\n",
    "    AUC is desired.\n",
    "    '''\n",
    "    # Plot\n",
    "    _, sorted_metric_aucs = get_sorted_videos_by_metric(arch, dataset_label, model_label, metric)\n",
    "    ax.plot(xrange(len(sorted_metric_aucs)), sorted_metric_aucs / float(max_value), label=label)\n",
    "\n",
    "\n",
    "def plot_arch_comparison(dataset_label, model_label, metric, max_value=1.0):\n",
    "    '''\n",
    "    Plot the ranked performance plot between each architecture for a given train/test set combination\n",
    "    '''\n",
    "    display(HTML('<hr style=\"height:2px\">'))\n",
    "    display(HTML('<h1>Train:%s<br>Test:%s</h1>' % (model_label, dataset_label)))\n",
    "    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    for arch in ARCHS:\n",
    "        plot_sorted_auc(ax1, arch, dataset_label, model_label, metric, arch, max_value=max_value)\n",
    "    ax1.set_xlabel('Instance #')\n",
    "    ax1.set_ylabel('%s AUC' % metric.upper())\n",
    "    ax1.legend(loc='lower left', bbox_to_anchor=(1, 0))\n",
    "    ax1.set_title('Architecture comparison\\nTraining set: %s\\nTesting set: %s' % (model_label, dataset_label))\n",
    "    plt.show()\n",
    "    \n",
    "    for arch in ARCHS:\n",
    "        display(Markdown('## Architecture: %s' % arch))\n",
    "        sorted_idx, _ = get_sorted_videos_by_metric(arch, dataset_label, model_label, metric)\n",
    "        visualize_videos(arch, dataset_label, model_label, sorted_idx)\n",
    "\n",
    "\n",
    "def plot_training_set_comparison(exps, dataset_label, metric, max_value=1.0):\n",
    "    '''\n",
    "    Plot performance of all models that were tested on the given dataset on one graph.\n",
    "    '''\n",
    "    \n",
    "    display(HTML('<hr style=\"height:2px\">'))\n",
    "    display(HTML('<h1>Test:%s</h1>' % (dataset_label)))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    \n",
    "    for arch in ARCHS:\n",
    "        if dataset_label not in exps[arch]:\n",
    "            warn('Did not find dataset %s for architecture %s, skipping' % (dataset_label, arch))\n",
    "            continue\n",
    "        model_labels = exps[arch][dataset_label]\n",
    "\n",
    "        # Plot\n",
    "        for model_label in model_labels:\n",
    "            plot_sorted_auc(ax1, arch, dataset_label, model_label, metric,\n",
    "                            'arch=%s,train=%s' % (arch, model_label), max_value=max_value)\n",
    "    \n",
    "    # Format plot\n",
    "    display(HTML('<h2>Sorted SSIM AUC graph</h2>'))\n",
    "    ax1.set_xlabel('Instance #')\n",
    "    ax1.set_ylabel('%s AUC' % metric.upper())\n",
    "    ax1.legend(loc='lower left', bbox_to_anchor=(1, 0))\n",
    "    ax1.set_title('Training set comparison\\nTesting set: %s' % dataset_label)\n",
    "    plt.show()\n",
    "    \n",
    "    for arch in ARCHS:\n",
    "        if dataset_label not in exps[arch]:\n",
    "            warn('Did not find dataset %s for architecture %s, skipping' % (dataset_label, arch))\n",
    "            continue\n",
    "        model_labels = exps[arch][dataset_label]\n",
    "\n",
    "        for model_label in model_labels:\n",
    "            # Show videos\n",
    "            display(HTML('<h2>Architecture: %s<br>Train: %s</h2>' % (arch, model_label)))\n",
    "            sorted_idx, _ = get_sorted_videos_by_metric(arch, dataset_label, model_label, metric)\n",
    "            visualize_videos(arch, dataset_label, model_label, sorted_idx)\n",
    "\n",
    "\n",
    "def summarize_experiment(arch, dataset_label, model_label, metric, max_value=1.0):\n",
    "    '''\n",
    "    Show the ranked AUC plot and videos for one experiment.\n",
    "    '''\n",
    "    display(HTML('<hr style=\"height:2px\">'))\n",
    "    display(HTML('<h2>Architecture: %s<br>Train:%s<br>Test:%s</h2>' \\\n",
    "                % (arch, model_label, dataset_label)))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    plot_sorted_auc(ax1, arch, dataset_label, model_label, metric, None, max_value=max_value)\n",
    "    ax1.set_xlabel('Instance #')\n",
    "    ax1.set_ylabel('%s AUC' % metric.upper())\n",
    "    plt.show()\n",
    "    \n",
    "    sorted_idx, _ = get_sorted_videos_by_metric(arch, dataset_label, model_label, metric)\n",
    "    visualize_videos(arch, dataset_label, model_label, sorted_idx)\n",
    "\n",
    "\n",
    "def summarize_discriminator_output(dataset_label, model_label, use_zscore=False):\n",
    "    '''\n",
    "    Plot the discriminator score of MCNet for the given training and test set.\n",
    "    Only valid for long-term video datasets\n",
    "    '''\n",
    "    assert(dataset_label.endswith('_long'))\n",
    "    \n",
    "    display(HTML('<hr style=\"height:2px\">'))\n",
    "    display(HTML('<h2>Train:%s<br>Test:%s</h2>' \\\n",
    "                % (model_label, dataset_label)))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    results = get_results('mcnet', dataset_label, model_label)\n",
    "    # Skip first column, which is zeros for some reason\n",
    "    disc_output = results['disc_output'][:, 1:]\n",
    "    if use_zscore:\n",
    "        disc_output = zscore(disc_output)\n",
    "    # Define zero-indexed frame indexes for discriminator output\n",
    "    frame_indexes = np.arange(disc_output.shape[1]) + 15 + 1\n",
    "    sorted_idx, _ = get_sorted_videos_by_metric('mcnet', dataset_label, model_label, 'ssim')\n",
    "    \n",
    "    for inv_rank in xrange(3):\n",
    "        video_idx = sorted_idx[inv_rank]\n",
    "        rank = len(sorted_idx) - inv_rank\n",
    "        disc_output_cur_vid = disc_output[video_idx, :]\n",
    "        ax1.plot(frame_indexes[::10], disc_output_cur_vid[::10], label='%04d (rank %d)' % (video_idx, rank))\n",
    "\n",
    "    for inv_rank in xrange(len(sorted_idx)-3,len(sorted_idx)):\n",
    "        video_idx = sorted_idx[inv_rank]\n",
    "        rank = len(sorted_idx) - inv_rank\n",
    "        disc_output_cur_vid = disc_output[video_idx, :]\n",
    "        ax1.plot(frame_indexes[::10], disc_output_cur_vid[::10], label='%04d (rank %d)' % (video_idx, rank), linestyle=':')\n",
    "\n",
    "    # Plot mean discriminator score over time\n",
    "    mean_output = np.mean(disc_output, axis=0)\n",
    "    ax1.plot(frame_indexes[::10], mean_output[::10], label='mean', linestyle='--')\n",
    "        \n",
    "    # Format plot\n",
    "    ax1.legend(loc='lower left', bbox_to_anchor=(1, 0))\n",
    "    ax1.set_xlim([16, 500])\n",
    "    ax1.set_xlabel('Frame index (0 = first GT frame)')\n",
    "    if use_zscore:\n",
    "        ax1.set_ylabel('Discriminator output (z-scored)')\n",
    "        ax1.set_title('Discriminator output for long-term videos (z-scored)\\nTraining set: %s\\nTesting set: %s' \\\n",
    "                     % (model_label, dataset_label) )\n",
    "    else:\n",
    "        ax1.set_ylabel('Discriminator output')\n",
    "        ax1.set_title('Discriminator output for long-term videos\\nTraining set: %s\\nTesting set: %s' \\\n",
    "                     % (model_label, dataset_label) )\n",
    "    plt.show()\n",
    "    \n",
    "    # Show examples\n",
    "    visualize_videos('mcnet', dataset_label, model_label, sorted_idx)\n",
    "\n",
    "def zscore(a):\n",
    "    mean = np.mean(a)\n",
    "    std = np.std(a)\n",
    "    return (a - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and validate the experiments on disk (MUST RUN BEFORE PLOTTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = load_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug individual plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_arch_comparison('flashing=async_b+num_digits=2', 'flashing=async_a+num_digits=2', 'ssim')\n",
    "# plot_training_set_comparison(experiments, 'flashing=async_b+num_digits=2', 'ssim')\n",
    "# summarize_experiment('mcnet', 'translation=on+num_digits=2', 'translation=on', 'ssim')\n",
    "# summarize_discriminator_output('rotation=no_limit+num_digits=2_long', 'rotation=no_limit', use_zscore=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short-term evaluation\n",
    "\n",
    "The cell below shows the results of short-term prediction (i.e. given the first 10 frames, predict the next 5) for each test set. Note that this evaluates over the same time-steps that the model was trained on. Each plot corresponds to one test dataset, and shows the ranked AUC plot for every model that was evaluated on that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset_labels = [x.replace('data=', '') for x in os.listdir(RESULTS_ROOTS['mcnet']) if not x.endswith('_long')]\n",
    "for dataset_label in test_dataset_labels:\n",
    "    plot_training_set_comparison(experiments, dataset_label, 'ssim', max_value=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-term evaluation\n",
    "\n",
    "The cell below shows the results of long-term prediction (i.e. given the first 10 frames, predict the next 490 frames) for each test set. Since the models were only trained to predict the next 5 frames, this explores how well the learned short-term dynamics fit the long-term dynamics. Each plot corresponds to one test dataset, and shows the ranked AUC plot for every model that was evaluated on that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset_labels = [x.replace('data=', '') for x in os.listdir(RESULTS_ROOTS['mcnet']) if x.endswith('_long')]\n",
    "for dataset_label in test_dataset_labels:\n",
    "    plot_training_set_comparison(experiments, dataset_label, 'ssim', max_value=485)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator scores for long-term prediction\n",
    "\n",
    "The MCNet discriminator takes as input a 15-frame sequence, and outputs a value in [0, 1] corresponding to the confidence that the input is a real (not-generated) sequence (i.e. a real sequence should have confidence 1). To generate the plots below, a sliding window provides input for the discriminator. The x-axis indicates the index of the last frame in the input sequence, and the y-axis is the z-score of the current output over all discriminator scores in the video set. Each plot shows the output for the worst five videos (solid lines) and the best five videos (dotted lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset_label in experiments['mcnet']:\n",
    "    if not dataset_label.endswith('_long'): continue\n",
    "    for model_label in experiments['mcnet'][dataset_label]:\n",
    "        summarize_discriminator_output(dataset_label, model_label, use_zscore=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
